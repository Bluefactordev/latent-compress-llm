--- START OF FILE latent_compress_llm_completo_001.txt ---
Generato il: Wed Dec 31 10:58:05 UTC 2025
Versione: 001
Sistema di Compressione Latente per Long-Context LLM
‚ö†Ô∏è IMPORTANTE: Sistema completo per compressione contesti molto lunghi (fino a ~262k token)
üì¶ ARCHITETTURA:
   ‚úÖ Reader/Teacher: Qwen3 30B MoE AWQ 4bit legge contesto completo (fino a 262k token)
   ‚úÖ Compressore: Resampler stile Perceiver che comprime contesto in sequenze latenti corte
   ‚úÖ Reasoner: Qwen3 4B consuma solo (latenti + domanda) per generare risposta
   ‚úÖ Baseline: Teacher full context, RAG top-k chunks, Summary per chunk, Pooling latents

=== START OF FILE README.md ===
Percorso: README.md
Dimensione: 1659 bytes
Ultima modifica: 2025-12-31 10:31:58.149614200 +0000

# Latent Compression for Long-Context LLM

Sistema di compressione latente per contesti molto lunghi (fino a ~262k token) utilizzando un modello Reader/Teacher (Qwen3 30B MoE AWQ) e un Reasoner pi√π piccolo (Qwen3 4B).

## Architettura

1. **Reader/Teacher**: Qwen3 30B MoE AWQ 4bit legge il contesto completo (fino a 262k token)
2. **Compressore**: Resampler stile Perceiver che comprime il contesto in sequenze di token latenti molto pi√π corte
3. **Reasoner**: Qwen3 4B consuma solo (latenti + domanda) per generare la risposta

## Struttura

```
latent-compress-llm/
  configs/          # Configurazioni YAML
  data/             # Dataset e cache
  src/              # Codice sorgente
  runs/             # Checkpoint training
  outputs/          # Report e plot
```

## Setup

```bash
pip install -r requirements.txt
```

## Utilizzo

### 1. Generare dataset sintetico

```bash
python -m src.data.generate_synth --config configs/base.yaml
```

### 2. Training compressore

```bash
python -m src.train.train_compressor --config configs/base.yaml --exp S0
```

### 3. Valutazione

```bash
python -m src.eval.run_eval --config configs/base.yaml --exp S0
python -m src.eval.report --input outputs/report.json
```

## Esperimenti

- **S0**: Sanity check - contesto 32k, N_lat 1024
- **S1**: Main - contesto 256k, N_lat {8192, 4096, 1024}

## Baseline

1. Teacher full context (vLLM)
2. RAG top-k chunks
3. Summary per chunk
4. Pooling latents (mean/max)

## Assunzioni

- Teacher disponibile su vLLM porta 8000
- Se estrazione hidden states fallisce con AWQ, si usa fallback logits-only o proxy model
- Reasoner usa Transformers per supportare inputs_embeds


=== END OF FILE README.md ===


=== START OF FILE requirements.txt ===
Percorso: requirements.txt
Dimensione: 277 bytes
Ultima modifica: 2025-12-31 10:31:58.093613917 +0000

torch>=2.0.0
transformers>=4.40.0
vllm>=0.4.0
accelerate>=0.30.0
datasets>=2.16.0
numpy>=1.24.0
scipy>=1.11.0
scikit-learn>=1.3.0
matplotlib>=3.8.0
pandas>=2.0.0
pyyaml>=6.0
tqdm>=4.66.0
sentencepiece>=0.1.99
bitsandbytes>=0.41.0
autoawq>=0.2.0
rank-bm25>=0.2.2
einops>=0.7.0


=== END OF FILE requirements.txt ===


=== START OF FILE configs/base.yaml ===
Percorso: configs/base.yaml
Dimensione: 1129 bytes
Ultima modifica: 2025-12-31 10:34:00.649212677 +0000

# Configurazione base per compressione latente

teacher_model: "Qwen/Qwen3-30B-A3B-AWQ"  # Teacher su vLLM porta 8000
reasoner_model: "Qwen/Qwen2.5-7B-Instruct"  # Reasoner pi√π piccolo
teacher_vllm_url: "http://localhost:8000/v1"

# Contesto
teacher_ctx: 262000
reasoner_ctx: 64000
chunk_size_tokens: 64000
chunk_overlap_tokens: 1024

# Compressore
n_latents: 4096
d_lat: 512
n_latents_global: 8192
use_global_resampler: true
n_compressor_blocks: 2
n_heads: 8
ff_dim: 2048

# Layer per hidden states
layer_indices: [-6]

# Training
train:
  lr: 2e-4
  batch_size: 1
  gradient_accumulation_steps: 8
  max_steps: 2000
  max_answer_tokens: 64
  warmup_steps: 100
  save_steps: 500
  eval_steps: 200
  checkpoint_dir: "runs"

# Eval
eval:
  temperature: 0.0
  max_new_tokens: 64
  top_p: 1.0

# Baseline
baselines:
  rag_k: [8, 16]
  summary_tokens_per_chunk: 512
  rag_chunk_size: 2048

# Dataset
data:
  train_size: 2000
  val_size: 200
  test_size: 500
  num_keys: 500
  noise_ratio: 0.90
  min_context_tokens: 16000
  max_context_tokens: 262000

# Paths
paths:
  data_dir: "data"
  output_dir: "outputs"
  cache_dir: ".cache"


=== END OF FILE configs/base.yaml ===


=== START OF FILE configs/exp_grid.yaml ===
Percorso: configs/exp_grid.yaml
Dimensione: 620 bytes
Ultima modifica: 2025-12-31 10:31:58.093613917 +0000

# Griglia esperimenti

experiments:
  S0:
    name: "sanity_check"
    context_tokens: 32000
    n_latents: 1024
    use_global_resampler: false
    train:
      max_steps: 2000
  
  S1_4096:
    name: "main_4096"
    context_tokens: 256000
    n_latents: 4096
    use_global_resampler: true
    train:
      max_steps: 5000
  
  S1_8192:
    name: "main_8192"
    context_tokens: 256000
    n_latents: 8192
    use_global_resampler: true
    train:
      max_steps: 5000
  
  S1_1024:
    name: "main_1024"
    context_tokens: 256000
    n_latents: 1024
    use_global_resampler: true
    train:
      max_steps: 5000


=== END OF FILE configs/exp_grid.yaml ===


=== START OF FILE src/__init__.py ===
Percorso: src/__init__.py
Dimensione: 26 bytes
Ultima modifica: 2025-12-31 10:31:58.093613917 +0000

# Latent Compression LLM


=== END OF FILE src/__init__.py ===


=== START OF FILE src/data/__init__.py ===
Percorso: src/data/__init__.py
Dimensione: 16 bytes
Ultima modifica: 2025-12-31 10:31:58.093613917 +0000

# Data modules


=== END OF FILE src/data/__init__.py ===


=== START OF FILE src/data/dataset.py ===
Percorso: src/data/dataset.py
Dimensione: 1876 bytes
Ultima modifica: 2025-12-31 10:31:58.093613917 +0000

"""
Dataset PyTorch per training e eval
"""
import json
from pathlib import Path
from typing import List, Dict, Optional
from torch.utils.data import Dataset
from transformers import AutoTokenizer

from src.utils.tokenization import chunk_text


class LatentCompressionDataset(Dataset):
    """Dataset per compressione latente"""
    
    def __init__(self, data_file: str, tokenizer, chunk_size: int, 
                 chunk_overlap: int = 0, max_examples: Optional[int] = None):
        """
        Args:
            data_file: Path a file JSONL
            tokenizer: Tokenizer per chunking
            chunk_size: Dimensione chunk in token
            chunk_overlap: Overlap tra chunk
            max_examples: Limita numero esempi (None = tutti)
        """
        self.tokenizer = tokenizer
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
        # Carica dati
        self.examples = []
        with open(data_file, 'r') as f:
            for line in f:
                if max_examples and len(self.examples) >= max_examples:
                    break
                self.examples.append(json.loads(line))
        
        print(f"Caricati {len(self.examples)} esempi da {data_file}")
    
    def __len__(self):
        return len(self.examples)
    
    def __getitem__(self, idx):
        example = self.examples[idx]
        
        # Chunking del contesto
        context = example["context"]
        chunks = chunk_text(context, self.tokenizer, self.chunk_size, self.chunk_overlap)
        
        return {
            "context_chunks": [chunk[0] for chunk in chunks],
            "chunk_positions": [(chunk[1], chunk[2]) for chunk in chunks],
            "question": example["question"],
            "answer": example["answer"],
            "metadata": example.get("metadata", {}),
            "_idx": idx
        }


=== END OF FILE src/data/dataset.py ===


=== START OF FILE src/data/generate_synth.py ===
Percorso: src/data/generate_synth.py
Dimensione: 12566 bytes
Ultima modifica: 2025-12-31 10:34:04.780232230 +0000

"""
Generatore di dataset sintetico per test compressione latente
"""
import json
import random
import argparse
from pathlib import Path
from typing import List, Dict, Tuple
import yaml
from tqdm import tqdm

from src.utils.seed import set_seed
from src.utils.tokenization import get_tokenizer, count_tokens


class SyntheticDataGenerator:
    """Genera documenti lunghi con facts nascosti"""
    
    def __init__(self, config: Dict, tokenizer):
        self.config = config
        self.tokenizer = tokenizer
        self.data_config = config.get('data', {})
        
    def generate_keys(self, num_keys: int) -> List[Tuple[str, str]]:
        """Genera chiavi-valore per needle facts"""
        keys = []
        for i in range(1, num_keys + 1):
            key = f"KEY_{i:04d}"
            # Valore casuale (stringa o numero)
            if random.random() < 0.5:
                value = f"VAL_{random.randint(1000, 9999)}"
            else:
                value = str(random.randint(100000, 999999))
            keys.append((key, value))
        return keys
    
    def generate_noise_text(self, num_tokens: int) -> str:
        """Genera testo noise per riempire il documento"""
        # Template di paragrafi casuali
        templates = [
            "The following section discusses various aspects of the topic at hand. "
            "It is important to consider multiple perspectives when analyzing complex systems. "
            "Research has shown that understanding the underlying mechanisms requires careful examination.",
            
            "In recent years, there has been significant progress in this field. "
            "Many researchers have contributed to our understanding of these phenomena. "
            "The implications of these findings extend beyond the immediate scope of investigation.",
            
            "A comprehensive analysis reveals several key patterns. "
            "These patterns emerge across different contexts and time periods. "
            "Further investigation is needed to fully understand their significance.",
            
            "The methodology employed in this study follows established best practices. "
            "Data collection procedures were designed to minimize potential biases. "
            "Statistical analysis was performed using standard techniques.",
            
            "Historical context provides important background for understanding current developments. "
            "Previous work in this area has laid the foundation for contemporary research. "
            "Future directions will build upon these established frameworks."
        ]
        
        text_parts = []
        current_tokens = 0
        
        while current_tokens < num_tokens:
            template = random.choice(templates)
            tokens = count_tokens(template, self.tokenizer)
            if current_tokens + tokens <= num_tokens:
                text_parts.append(template)
                current_tokens += tokens
            else:
                # Tronca l'ultimo template
                remaining = num_tokens - current_tokens
                # Approssimazione: prendi primi N caratteri
                chars_per_token = len(template) / tokens
                chars_needed = int(remaining * chars_per_token)
                text_parts.append(template[:chars_needed])
                break
        
        return " ".join(text_parts)
    
    def generate_graph_facts(self, num_facts: int) -> List[Dict]:
        """Genera fatti multi-hop con relazioni"""
        facts = []
        entities_a = [f"A_{i}" for i in range(1, 100)]
        entities_b = [f"B_{i}" for i in range(1, 100)]
        entities_c = [f"C_{i}" for i in range(1, 50)]
        
        for _ in range(num_facts):
            a = random.choice(entities_a)
            b = random.choice(entities_b)
            c = random.choice(entities_c)
            
            fact1 = {
                "type": "parent",
                "entity1": a,
                "entity2": b,
                "text": f"{a} is parent of {b}."
            }
            fact2 = {
                "type": "works_at",
                "entity1": b,
                "entity2": c,
                "text": f"{b} works at {c}."
            }
            
            question = f"Where does {a}'s child work?"
            answer = c
            
            facts.append({
                "facts": [fact1, fact2],
                "question": question,
                "answer": answer,
                "hops": 2
            })
        
        return facts
    
    def insert_fact_into_document(self, doc_parts: List[str], fact_text: str, 
                                  target_position: int, total_tokens: int) -> List[str]:
        """Inserisce un fact in una posizione specifica del documento"""
        # Calcola posizione approssimativa
        current_tokens = 0
        insert_idx = 0
        
        for i, part in enumerate(doc_parts):
            part_tokens = count_tokens(part, self.tokenizer)
            if current_tokens + part_tokens >= target_position:
                insert_idx = i
                break
            current_tokens += part_tokens
        
        # Inserisci fact
        fact_with_context = f" {fact_text} "
        doc_parts.insert(insert_idx, fact_with_context)
        return doc_parts
    
    def generate_document(self, target_tokens: int, keys: List[Tuple[str, str]], 
                         graph_facts: List[Dict], noise_ratio: float = 0.90) -> Tuple[str, Dict]:
        """Genera un documento con facts nascosti"""
        # Calcola token per facts vs noise
        fact_tokens_estimate = len(keys) * 20 + len(graph_facts) * 30  # Stima
        noise_tokens = int((target_tokens - fact_tokens_estimate) * noise_ratio)
        
        # Genera noise base
        noise_text = self.generate_noise_text(noise_tokens)
        doc_parts = [noise_text]
        
        metadata = {
            "key_positions": {},
            "graph_fact_positions": [],
            "versions": {}
        }
        
        # Inserisci keys in posizioni casuali
        key_insertions = []
        for key, value in keys:
            # Decidi se inserire versione multipla
            if random.random() < 0.3:  # 30% hanno versioni multiple
                # Versione vecchia
                old_value = f"VAL_{random.randint(1000, 9999)}"
                old_pos = random.randint(0, target_tokens // 3)
                key_insertions.append(("old", key, old_value, old_pos))
                metadata["versions"][key] = {"old": old_value, "new": value}
            
            # Versione corretta
            pos = random.randint(target_tokens // 2, target_tokens - 1000)
            key_insertions.append(("new", key, value, pos))
        
        # Ordina per posizione
        key_insertions.sort(key=lambda x: x[3])
        
        # Inserisci keys
        for version, key, value, pos in key_insertions:
            fact_text = f"{key} = {value}"
            doc_parts = self.insert_fact_into_document(doc_parts, fact_text, pos, target_tokens)
            if version == "new":
                metadata["key_positions"][key] = pos
        
        # Inserisci graph facts
        for graph_fact in graph_facts[:5]:  # Limita a 5 per documento
            for fact in graph_fact["facts"]:
                pos = random.randint(0, target_tokens - 100)
                doc_parts = self.insert_fact_into_document(
                    doc_parts, fact["text"], pos, target_tokens
                )
                metadata["graph_fact_positions"].append({
                    "fact": fact["text"],
                    "position": pos
                })
        
        # Ricompone documento
        full_doc = " ".join(doc_parts)
        
        # Verifica lunghezza reale
        actual_tokens = count_tokens(full_doc, self.tokenizer)
        metadata["actual_tokens"] = actual_tokens
        metadata["target_tokens"] = target_tokens
        
        return full_doc, metadata
    
    def generate_question_answer(self, keys: List[Tuple[str, str]], 
                                graph_facts: List[Dict], metadata: Dict) -> Tuple[str, str, Dict]:
        """Genera domanda e risposta per il documento"""
        qa_type = random.choice(["needle", "graph", "versioning"])
        
        if qa_type == "needle" and keys:
            # Domanda su key
            key, value = random.choice(keys)
            question = f"What is the value of {key}?"
            answer = value
            qa_metadata = {"type": "needle", "key": key, "distance": metadata["key_positions"].get(key, -1)}
        
        elif qa_type == "graph" and graph_facts:
            # Domanda multi-hop
            graph_fact = random.choice(graph_facts)
            question = graph_fact["question"]
            answer = graph_fact["answer"]
            qa_metadata = {"type": "graph", "hops": graph_fact["hops"]}
        
        else:
            # Versioning: chiedi latest
            if metadata.get("versions"):
                key = random.choice(list(metadata["versions"].keys()))
                question = f"What is the latest value of {key}?"
                answer = metadata["versions"][key]["new"]
                qa_metadata = {"type": "versioning", "key": key}
            else:
                # Fallback a needle
                key, value = random.choice(keys)
                question = f"What is the value of {key}?"
                answer = value
                qa_metadata = {"type": "needle", "key": key}
        
        return question, answer, qa_metadata
    
    def generate_example(self, target_tokens: int) -> Dict:
        """Genera un esempio completo"""
        num_keys = self.data_config.get("num_keys", 500)
        num_graph_facts = 10
        noise_ratio = self.data_config.get("noise_ratio", 0.90)
        
        keys = self.generate_keys(num_keys)
        graph_facts = self.generate_graph_facts(num_graph_facts)
        
        context, metadata = self.generate_document(
            target_tokens, keys, graph_facts, noise_ratio
        )
        
        question, answer, qa_metadata = self.generate_question_answer(keys, graph_facts, metadata)
        
        return {
            "context": context,
            "question": question,
            "answer": answer,
            "metadata": {**metadata, "qa": qa_metadata}
        }


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=str, default="configs/base.yaml")
    parser.add_argument("--output", type=str, default="data/raw/synthetic.jsonl")
    parser.add_argument("--seed", type=int, default=42)
    args = parser.parse_args()
    
    set_seed(args.seed)
    
    # Carica config
    with open(args.config, 'r') as f:
        config = yaml.safe_load(f)
    
    # Carica tokenizer teacher
    # Usa Qwen2.5 per tokenizer (compatibile con Qwen3)
    teacher_model = config["teacher_model"]
    print(f"Caricamento tokenizer compatibile Qwen...")
    try:
        tokenizer = get_tokenizer(teacher_model)
    except Exception as e:
        print(f"Errore caricamento {teacher_model}: {e}")
        print("Tentativo con Qwen2.5 (tokenizer compatibile)...")
        tokenizer = get_tokenizer("Qwen/Qwen2.5-7B-Instruct")
    
    # Crea generator
    generator = SyntheticDataGenerator(config, tokenizer)
    
    # Genera dataset
    data_config = config.get("data", {})
    train_size = data_config.get("train_size", 2000)
    val_size = data_config.get("val_size", 2000)
    test_size = data_config.get("test_size", 500)
    
    # Target tokens per split
    splits = {
        "train": (train_size, data_config.get("min_context_tokens", 16000)),
        "val": (val_size, data_config.get("min_context_tokens", 16000)),
        "test": (test_size, data_config.get("max_context_tokens", 262000))
    }
    
    output_dir = Path(args.output).parent
    output_dir.mkdir(parents=True, exist_ok=True)
    
    for split_name, (size, target_tokens) in splits.items():
        output_file = output_dir / f"synthetic_{split_name}.jsonl"
        print(f"\nGenerando {split_name}: {size} esempi, target {target_tokens} token...")
        
        with open(output_file, 'w') as f:
            for i in tqdm(range(size)):
                example = generator.generate_example(target_tokens)
                f.write(json.dumps(example) + '\n')
        
        print(f"Salvato in {output_file}")
    
    print("\nGenerazione completata!")


if __name__ == "__main__":
    main()


=== END OF FILE src/data/generate_synth.py ===


=== START OF FILE src/models/__init__.py ===
Percorso: src/models/__init__.py
Dimensione: 10 bytes
Ultima modifica: 2025-12-31 10:31:58.093613917 +0000

# Models


=== END OF FILE src/models/__init__.py ===


=== START OF FILE src/models/compressor.py ===
Percorso: src/models/compressor.py
Dimensione: 10596 bytes
Ultima modifica: 2025-12-31 10:31:58.112614013 +0000

"""
Compressore Perceiver-style per compressione latente
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional
import logging

logger = logging.getLogger(__name__)


class RMSNorm(nn.Module):
    """Root Mean Square Layer Normalization"""
    
    def __init__(self, dim: int, eps: float = 1e-6):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        norm = x.norm(dim=-1, keepdim=True) * (x.shape[-1] ** -0.5)
        return self.weight * x / (norm + self.eps)


class SwiGLU(nn.Module):
    """SwiGLU activation"""
    
    def __init__(self, dim: int):
        super().__init__()
        self.linear_gate = nn.Linear(dim, dim)
        self.linear = nn.Linear(dim, dim)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        gate = F.silu(self.linear_gate(x))
        return gate * self.linear(x)


class CrossAttentionBlock(nn.Module):
    """Cross-attention: Q da latents, K/V da teacher hidden states"""
    
    def __init__(self, d_lat: int, d_teacher: int, n_heads: int = 8):
        super().__init__()
        self.d_lat = d_lat
        self.d_teacher = d_teacher
        self.n_heads = n_heads
        self.head_dim = d_lat // n_heads
        
        assert d_lat % n_heads == 0, "d_lat deve essere divisibile per n_heads"
        
        # Proiezione teacher hidden states a d_lat
        self.teacher_proj = nn.Linear(d_teacher, d_lat)
        
        # Q da latents
        self.q_proj = nn.Linear(d_lat, d_lat)
        # K, V da teacher states proiettati
        self.k_proj = nn.Linear(d_lat, d_lat)
        self.v_proj = nn.Linear(d_lat, d_lat)
        self.out_proj = nn.Linear(d_lat, d_lat)
        
        self.norm = RMSNorm(d_lat)
    
    def forward(self, latents: torch.Tensor, teacher_states: torch.Tensor) -> torch.Tensor:
        """
        Args:
            latents: [N_lat, d_lat]
            teacher_states: [seq_len, d_teacher]
        
        Returns:
            [N_lat, d_lat]
        """
        # Proietta teacher states
        teacher_proj = self.teacher_proj(teacher_states)  # [seq_len, d_lat]
        
        # Q da latents
        q = self.q_proj(latents)  # [N_lat, d_lat]
        # K, V da teacher
        k = self.k_proj(teacher_proj)  # [seq_len, d_lat]
        v = self.v_proj(teacher_proj)  # [seq_len, d_lat]
        
        # Reshape per multi-head
        N_lat = latents.shape[0]
        seq_len = teacher_states.shape[0]
        
        q = q.view(N_lat, self.n_heads, self.head_dim)  # [N_lat, n_heads, head_dim]
        k = k.view(seq_len, self.n_heads, self.head_dim)  # [seq_len, n_heads, head_dim]
        v = v.view(seq_len, self.n_heads, self.head_dim)  # [seq_len, n_heads, head_dim]
        
        # Transpose per attention
        q = q.transpose(0, 1)  # [n_heads, N_lat, head_dim]
        k = k.transpose(0, 1)  # [n_heads, seq_len, head_dim]
        v = v.transpose(0, 1)  # [n_heads, seq_len, head_dim]
        
        # Attention
        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)
        attn_weights = F.softmax(scores, dim=-1)  # [n_heads, N_lat, seq_len]
        attn_output = torch.matmul(attn_weights, v)  # [n_heads, N_lat, head_dim]
        
        # Concatena heads
        attn_output = attn_output.transpose(0, 1).contiguous()  # [N_lat, n_heads, head_dim]
        attn_output = attn_output.view(N_lat, self.d_lat)  # [N_lat, d_lat]
        
        # Output projection
        output = self.out_proj(attn_output)
        
        # Residual + norm
        output = self.norm(output + latents)
        
        return output


class SelfAttentionBlock(nn.Module):
    """Self-attention sui latents"""
    
    def __init__(self, d_lat: int, n_heads: int = 8):
        super().__init__()
        self.d_lat = d_lat
        self.n_heads = n_heads
        self.head_dim = d_lat // n_heads
        
        self.q_proj = nn.Linear(d_lat, d_lat)
        self.k_proj = nn.Linear(d_lat, d_lat)
        self.v_proj = nn.Linear(d_lat, d_lat)
        self.out_proj = nn.Linear(d_lat, d_lat)
        
        self.norm = RMSNorm(d_lat)
    
    def forward(self, latents: torch.Tensor) -> torch.Tensor:
        """
        Args:
            latents: [N_lat, d_lat]
        
        Returns:
            [N_lat, d_lat]
        """
        N_lat = latents.shape[0]
        
        q = self.q_proj(latents)
        k = self.k_proj(latents)
        v = self.v_proj(latents)
        
        # Reshape
        q = q.view(N_lat, self.n_heads, self.head_dim).transpose(0, 1)
        k = k.view(N_lat, self.n_heads, self.head_dim).transpose(0, 1)
        v = v.view(N_lat, self.n_heads, self.head_dim).transpose(0, 1)
        
        # Attention
        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)
        attn_weights = F.softmax(scores, dim=-1)
        attn_output = torch.matmul(attn_weights, v)
        
        # Concatena
        attn_output = attn_output.transpose(0, 1).contiguous().view(N_lat, self.d_lat)
        output = self.out_proj(attn_output)
        
        # Residual + norm
        output = self.norm(output + latents)
        
        return output


class CompressorBlock(nn.Module):
    """Blocco completo: cross-attn + self-attn + FF"""
    
    def __init__(self, d_lat: int, d_teacher: int, n_heads: int = 8, ff_dim: int = 2048):
        super().__init__()
        self.cross_attn = CrossAttentionBlock(d_lat, d_teacher, n_heads)
        self.self_attn = SelfAttentionBlock(d_lat, n_heads)
        self.ff = nn.Sequential(
            nn.Linear(d_lat, ff_dim),
            SwiGLU(ff_dim),
            nn.Linear(ff_dim, d_lat)
        )
        self.norm = RMSNorm(d_lat)
    
    def forward(self, latents: torch.Tensor, teacher_states: torch.Tensor) -> torch.Tensor:
        # Cross-attention
        latents = self.cross_attn(latents, teacher_states)
        # Self-attention
        latents = self.self_attn(latents)
        # Feed-forward
        ff_out = self.ff(latents)
        latents = self.norm(latents + ff_out)
        return latents


class GlobalResampler(nn.Module):
    """Second-stage resampler per latents globali"""
    
    def __init__(self, d_lat: int, n_global: int, n_heads: int = 8):
        super().__init__()
        self.n_global = n_global
        self.d_lat = d_lat
        
        # Latents globali learnable
        self.global_latents = nn.Parameter(torch.randn(n_global, d_lat) * 0.02)
        
        # Cross-attn da global a chunk latents
        self.cross_attn = CrossAttentionBlock(d_lat, d_lat, n_heads)
        self.self_attn = SelfAttentionBlock(d_lat, n_heads)
        self.ff = nn.Sequential(
            nn.Linear(d_lat, d_lat * 4),
            SwiGLU(d_lat * 4),
            nn.Linear(d_lat * 4, d_lat)
        )
        self.norm = RMSNorm(d_lat)
    
    def forward(self, chunk_latents: torch.Tensor) -> torch.Tensor:
        """
        Args:
            chunk_latents: [num_chunks * N_lat, d_lat] (concatenati)
        
        Returns:
            [n_global, d_lat]
        """
        # Cross-attention da global latents a chunk latents
        global_lat = self.global_latents  # [n_global, d_lat]
        global_lat = self.cross_attn(global_lat, chunk_latents)
        global_lat = self.self_attn(global_lat)
        ff_out = self.ff(global_lat)
        global_lat = self.norm(global_lat + ff_out)
        return global_lat


class LatentCompressor(nn.Module):
    """Compressore principale"""
    
    def __init__(self, config: dict):
        super().__init__()
        self.config = config
        self.n_latents = config["n_latents"]
        self.d_lat = config["d_lat"]
        self.d_teacher = config.get("d_teacher", 4096)  # Default per Qwen
        self.n_blocks = config.get("n_compressor_blocks", 2)
        self.n_heads = config.get("n_heads", 8)
        self.ff_dim = config.get("ff_dim", 2048)
        self.use_global_resampler = config.get("use_global_resampler", False)
        self.n_global = config.get("n_latents_global", 8192)
        
        # Latent tokens iniziali
        self.latent_tokens = nn.Parameter(
            torch.randn(self.n_latents, self.d_lat) * 0.02
        )
        
        # Embeddings posizionali
        self.chunk_id_embedding = nn.Embedding(1000, self.d_lat)  # Max 1000 chunk
        self.latent_idx_embedding = nn.Embedding(self.n_latents, self.d_lat)
        
        # Blocchi compressore
        self.blocks = nn.ModuleList([
            CompressorBlock(self.d_lat, self.d_teacher, self.n_heads, self.ff_dim)
            for _ in range(self.n_blocks)
        ])
        
        # Global resampler opzionale
        if self.use_global_resampler:
            self.global_resampler = GlobalResampler(self.d_lat, self.n_global, self.n_heads)
        else:
            self.global_resampler = None
    
    def forward(self, teacher_states_list: list, chunk_ids: Optional[list] = None) -> torch.Tensor:
        """
        Args:
            teacher_states_list: Lista di [seq_len_i, d_teacher] per ogni chunk
            chunk_ids: Lista di ID chunk (opzionale)
        
        Returns:
            Latents finali: [total_latents, d_lat]
        """
        all_latents = []
        
        for chunk_idx, teacher_states in enumerate(teacher_states_list):
            # Inizializza latents per questo chunk
            latents = self.latent_tokens.clone()  # [N_lat, d_lat]
            
            # Aggiungi embeddings posizionali
            if chunk_ids is not None:
                chunk_id = chunk_ids[chunk_idx]
            else:
                chunk_id = chunk_idx
            
            chunk_emb = self.chunk_id_embedding(torch.tensor(chunk_id, device=latents.device))
            latent_indices = torch.arange(self.n_latents, device=latents.device)
            latent_emb = self.latent_idx_embedding(latent_indices)
            
            latents = latents + chunk_emb.unsqueeze(0) + latent_emb
            
            # Applica blocchi compressore
            for block in self.blocks:
                latents = block(latents, teacher_states)
            
            all_latents.append(latents)
        
        # Concatena tutti i latents
        global_latents = torch.cat(all_latents, dim=0)  # [num_chunks * N_lat, d_lat]
        
        # Applica global resampler se richiesto
        if self.global_resampler is not None:
            global_latents = self.global_resampler(global_latents)
        
        return global_latents


=== END OF FILE src/models/compressor.py ===


=== START OF FILE src/models/teacher_reader.py ===
Percorso: src/models/teacher_reader.py
Dimensione: 8346 bytes
Ultima modifica: 2025-12-31 10:31:58.093613917 +0000

"""
Teacher/Reader model: vLLM per generazione, Transformers per hidden states
"""
import json
import requests
from pathlib import Path
from typing import List, Optional, Tuple
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import logging

logger = logging.getLogger(__name__)


class TeacherReader:
    """Wrapper per teacher model: vLLM per generazione, Transformers per features"""
    
    def __init__(self, config: dict):
        self.config = config
        self.model_name = config["teacher_model"]
        self.vllm_url = config.get("teacher_vllm_url", "http://localhost:8000/v1")
        self.tokenizer = None
        self.transformers_model = None
        self.hidden_states_available = False
        
        # Carica tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name, trust_remote_code=True
        )
        
        # Prova a caricare Transformers per hidden states
        self._load_transformers_model()
    
    def _load_transformers_model(self):
        """Tenta di caricare modello Transformers per hidden states"""
        try:
            logger.info(f"Tentativo di caricare {self.model_name} in Transformers per hidden states...")
            
            # Prova prima con AWQ se supportato
            try:
                from awq import AutoAWQForCausalLM
                logger.info("Tentativo con AutoAWQ...")
                self.transformers_model = AutoAWQForCausalLM.from_quantized(
                    self.model_name,
                    trust_remote_code=True,
                    device_map="auto"
                )
                self.hidden_states_available = True
                logger.info("‚úì Modello AWQ caricato con successo")
                return
            except Exception as e:
                logger.warning(f"AutoAWQ fallito: {e}")
            
            # Fallback: prova con quantizzazione 8-bit
            try:
                logger.info("Tentativo con quantizzazione 8-bit...")
                quantization_config = BitsAndBytesConfig(
                    load_in_8bit=True,
                    device_map="auto"
                )
                self.transformers_model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    quantization_config=quantization_config,
                    trust_remote_code=True,
                    torch_dtype=torch.float16
                )
                self.hidden_states_available = True
                logger.info("‚úì Modello 8-bit caricato con successo")
                return
            except Exception as e:
                logger.warning(f"Quantizzazione 8-bit fallita: {e}")
            
            # Fallback finale: prova modello pi√π piccolo come proxy
            logger.warning("Tentativo con modello proxy pi√π piccolo...")
            proxy_model_name = "Qwen/Qwen2.5-7B-Instruct"  # Modello pi√π piccolo
            try:
                self.transformers_model = AutoModelForCausalLM.from_pretrained(
                    proxy_model_name,
                    trust_remote_code=True,
                    torch_dtype=torch.float16,
                    device_map="auto"
                )
                self.hidden_states_available = True
                logger.warning(f"‚ö† Usando modello proxy {proxy_model_name} per hidden states")
                logger.warning("Le hidden states potrebbero non corrispondere esattamente al teacher")
                return
            except Exception as e:
                logger.error(f"Anche modello proxy fallito: {e}")
            
            logger.error("‚ùå Impossibile caricare modello per hidden states. Useremo solo logits.")
            self.hidden_states_available = False
            
        except Exception as e:
            logger.error(f"Errore nel caricamento Transformers: {e}")
            self.hidden_states_available = False
    
    def generate_answer_vllm(self, context: str, question: str, 
                           max_tokens: int = 64, temperature: float = 0.0,
                           cache_dir: Optional[Path] = None) -> str:
        """
        Genera risposta usando vLLM API
        
        Args:
            context: Contesto completo
            question: Domanda
            max_tokens: Max token da generare
            temperature: Temperature sampling
            cache_dir: Directory per cache (opzionale)
        
        Returns:
            Risposta generata
        """
        # Controlla cache
        if cache_dir:
            cache_file = cache_dir / f"teacher_answer_{hash(context + question)}.json"
            if cache_file.exists():
                with open(cache_file, 'r') as f:
                    cached = json.load(f)
                    logger.debug(f"Risposta da cache: {cache_file}")
                    return cached["answer"]
        
        # Prompt
        prompt = f"{context}\n\nQuestion: {question}\nAnswer:"
        
        # Chiama vLLM API
        payload = {
            "model": "default",
            "prompt": prompt,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "stop": ["\n\n", "Question:"]
        }
        
        try:
            response = requests.post(
                f"{self.vllm_url}/completions",
                json=payload,
                timeout=300
            )
            response.raise_for_status()
            result = response.json()
            answer = result["choices"][0]["text"].strip()
            
            # Salva in cache
            if cache_dir:
                cache_file.parent.mkdir(parents=True, exist_ok=True)
                with open(cache_file, 'w') as f:
                    json.dump({"answer": answer, "prompt": prompt}, f)
            
            return answer
        
        except Exception as e:
            logger.error(f"Errore nella chiamata vLLM: {e}")
            raise
    
    def get_hidden_states(self, text_chunk: str, layer_indices: List[int] = [-6]) -> Optional[torch.Tensor]:
        """
        Estrae hidden states da un chunk di testo
        
        Args:
            text_chunk: Testo da processare
            layer_indices: Indici layer da estrarre (negativi = dall'ultimo)
        
        Returns:
            Tensor [seq_len, hidden_dim] o None se non disponibile
        """
        if not self.hidden_states_available or self.transformers_model is None:
            return None
        
        try:
            self.transformers_model.eval()
            
            # Tokenizza
            inputs = self.tokenizer(
                text_chunk,
                return_tensors="pt",
                truncation=True,
                max_length=self.config.get("teacher_ctx", 262000)
            )
            
            # Sposta su device
            device = next(self.transformers_model.parameters()).device
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # Forward pass
            with torch.no_grad():
                outputs = self.transformers_model(**inputs, output_hidden_states=True)
            
            # Estrai hidden states dai layer specificati
            hidden_states = outputs.hidden_states
            
            # Converti indici negativi
            num_layers = len(hidden_states)
            actual_indices = [
                i if i >= 0 else num_layers + i
                for i in layer_indices
            ]
            
            # Prendi l'ultimo layer richiesto (o media se multipli)
            selected_states = hidden_states[actual_indices[-1]]
            
            # Rimuovi batch dimension
            return selected_states[0]  # [seq_len, hidden_dim]
        
        except Exception as e:
            logger.error(f"Errore nell'estrazione hidden states: {e}")
            return None
    
    def batch_generate_answers(self, examples: List[dict], cache_dir: Optional[Path] = None) -> List[str]:
        """Genera risposte in batch (sequenziale per ora)"""
        answers = []
        for example in examples:
            answer = self.generate_answer_vllm(
                example["context"],
                example["question"],
                cache_dir=cache_dir
            )
            answers.append(answer)
        return answers


=== END OF FILE src/models/teacher_reader.py ===


=== START OF FILE src/models/reasoner_wrapper.py ===
Percorso: src/models/reasoner_wrapper.py
Dimensione: 8807 bytes
Ultima modifica: 2025-12-31 10:31:58.112614013 +0000

"""
Wrapper per reasoner model con supporto inputs_embeds
"""
import torch
import torch.nn as nn
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from typing import Optional
import logging

logger = logging.getLogger(__name__)


class ReasonerWrapper:
    """Wrapper per reasoner che accetta latents come embeddings"""
    
    def __init__(self, config: dict):
        self.config = config
        self.model_name = config["reasoner_model"]
        self.max_ctx = config.get("reasoner_ctx", 64000)
        
        # Carica tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name, trust_remote_code=True
        )
        
        # Carica modello con quantizzazione se necessario
        try:
            # Prova prima senza quantizzazione
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                trust_remote_code=True,
                torch_dtype=torch.float16,
                device_map="auto"
            )
            logger.info(f"Modello reasoner caricato senza quantizzazione")
        except Exception as e:
            logger.warning(f"Caricamento senza quantizzazione fallito: {e}")
            logger.info("Tentativo con quantizzazione 8-bit...")
            try:
                quantization_config = BitsAndBytesConfig(load_in_8bit=True)
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    quantization_config=quantization_config,
                    trust_remote_code=True,
                    device_map="auto"
                )
                logger.info("Modello reasoner caricato con quantizzazione 8-bit")
            except Exception as e2:
                logger.error(f"Anche quantizzazione fallita: {e2}")
                raise
        
        self.model.eval()
        self.device = next(self.model.parameters()).device
    
    def forward_with_latents(self, latent_embeds: torch.Tensor, question_text: str,
                            target_answer_text: Optional[str] = None) -> dict:
        """
        Forward pass con latents come embeddings
        
        Args:
            latent_embeds: [N_lat, d_emb] embeddings latenti
            question_text: Testo domanda
            target_answer_text: Risposta target (opzionale, per loss)
        
        Returns:
            dict con 'loss' (se target fornito) e 'logits'
        """
        # Tokenizza domanda
        question_inputs = self.tokenizer(
            question_text,
            return_tensors="pt",
            add_special_tokens=False
        )
        question_ids = question_inputs["input_ids"].to(self.device)  # [1, q_len]
        
        # Ottieni embeddings della domanda
        question_embeds = self.model.get_input_embeddings()(question_ids)  # [1, q_len, d_emb]
        
        # Prepara input: [latents] + [question]
        # Latents: [N_lat, d_emb] -> [1, N_lat, d_emb]
        latent_embeds = latent_embeds.unsqueeze(0)  # [1, N_lat, d_emb]
        
        # Concatena
        inputs_embeds = torch.cat([latent_embeds, question_embeds], dim=1)  # [1, N_lat + q_len, d_emb]
        
        # Attention mask
        n_lat = latent_embeds.shape[1]
        q_len = question_embeds.shape[1]
        attention_mask = torch.ones(1, n_lat + q_len, device=self.device, dtype=torch.long)
        
        # Se abbiamo target, tokenizza anche quello
        if target_answer_text is not None:
            # Tokenizza risposta target
            answer_inputs = self.tokenizer(
                target_answer_text,
                return_tensors="pt",
                add_special_tokens=False
            )
            answer_ids = answer_inputs["input_ids"].to(self.device)  # [1, a_len]
            
            # Concatena input_ids per calcolare loss
            # Per loss, dobbiamo allineare: input = [latents + question], labels = [question + answer]
            # Ma in realt√† vogliamo loss solo sulla parte answer
            # Usiamo un approccio pi√π semplice: prependiamo un token speciale per i latents
            
            # Forward con inputs_embeds
            outputs = self.model(
                inputs_embeds=inputs_embeds,
                attention_mask=attention_mask,
                labels=None  # Calcoliamo loss manualmente
            )
            
            # Estrai logits
            logits = outputs.logits  # [1, seq_len, vocab_size]
            
            # Per loss: vogliamo predire answer tokens
            # Logits shape: [1, n_lat + q_len, vocab_size]
            # Prendi logits dalla fine della question (ultimo token di question predice primo di answer)
            # Shift: logits[i] predice token[i+1]
            seq_len = logits.shape[1]
            answer_len = answer_ids.shape[1]
            
            # Prendi logits per predire answer (dall'ultimo token di question in poi)
            if n_lat + q_len - 1 + answer_len <= seq_len:
                answer_logits = logits[0, n_lat + q_len - 1: n_lat + q_len - 1 + answer_len, :]
            else:
                # Se non ci sono abbastanza logits, prendi quelli disponibili
                available = seq_len - (n_lat + q_len - 1)
                answer_logits = logits[0, n_lat + q_len - 1:, :]
                answer_ids = answer_ids[:, :available]
            
            # Calcola loss
            loss_fn = nn.CrossEntropyLoss()
            loss = loss_fn(
                answer_logits.view(-1, answer_logits.shape[-1]),
                answer_ids.view(-1)
            )
            
            return {"loss": loss, "logits": logits}
        else:
            # Solo forward, no loss
            outputs = self.model(
                inputs_embeds=inputs_embeds,
                attention_mask=attention_mask
            )
            return {"logits": outputs.logits}
    
    def generate_with_latents(self, latent_embeds: torch.Tensor, question_text: str,
                              max_new_tokens: int = 64, temperature: float = 0.0) -> str:
        """
        Genera risposta usando latents
        
        Args:
            latent_embeds: [N_lat, d_emb]
            question_text: Domanda
            max_new_tokens: Max token da generare
            temperature: Temperature sampling
        
        Returns:
            Risposta generata
        """
        # Tokenizza domanda
        question_inputs = self.tokenizer(
            question_text,
            return_tensors="pt",
            add_special_tokens=False
        )
        question_ids = question_inputs["input_ids"].to(self.device)
        question_embeds = self.model.get_input_embeddings()(question_ids)
        
        # Prepara inputs_embeds
        latent_embeds = latent_embeds.unsqueeze(0)
        inputs_embeds = torch.cat([latent_embeds, question_embeds], dim=1)
        
        n_lat = latent_embeds.shape[1]
        q_len = question_embeds.shape[1]
        attention_mask = torch.ones(1, n_lat + q_len, device=self.device, dtype=torch.long)
        
        # Genera
        with torch.no_grad():
            # Usa generate con inputs_embeds
            # Nota: transformers supporta inputs_embeds in generate
            try:
                generated_ids = self.model.generate(
                    inputs_embeds=inputs_embeds,
                    attention_mask=attention_mask,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature if temperature > 0 else None,
                    do_sample=temperature > 0,
                    pad_token_id=self.tokenizer.eos_token_id if self.tokenizer.eos_token_id is not None else self.tokenizer.pad_token_id
                )
            except Exception as e:
                logger.warning(f"Generate con inputs_embeds fallito: {e}, provo approccio alternativo")
                # Fallback: usa solo question senza latents
                generated_ids = self.model.generate(
                    question_ids,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature if temperature > 0 else None,
                    do_sample=temperature > 0,
                    pad_token_id=self.tokenizer.eos_token_id if self.tokenizer.eos_token_id is not None else self.tokenizer.pad_token_id
                )
                # Decodifica solo la parte generata
                answer = self.tokenizer.decode(generated_ids[0][question_ids.shape[1]:], skip_special_tokens=True)
                return answer.strip()
        
        # Decodifica solo la parte generata (dopo latents + question)
        generated_tokens = generated_ids[0, n_lat + q_len:]
        answer = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
        
        return answer.strip()


=== END OF FILE src/models/reasoner_wrapper.py ===


=== START OF FILE src/models/rag_baseline.py ===
Percorso: src/models/rag_baseline.py
Dimensione: 1605 bytes
Ultima modifica: 2025-12-31 10:31:58.112614013 +0000

"""
Baseline RAG: retrieval top-k chunks
"""
import torch
from typing import List
from rank_bm25 import BM25Okapi
import logging

logger = logging.getLogger(__name__)


class RAGBaseline:
    """RAG baseline con BM25 retrieval"""
    
    def __init__(self, config: dict):
        self.config = config
        self.rag_k = config["baselines"]["rag_k"]
        self.chunk_size = config["baselines"].get("rag_chunk_size", 2048)
    
    def retrieve_chunks(self, question: str, context_chunks: List[str], k: int) -> List[str]:
        """
        Retrieval top-k chunks usando BM25
        
        Args:
            question: Domanda
            context_chunks: Lista di chunk di contesto
            k: Numero di chunk da recuperare
        
        Returns:
            Lista di chunk recuperati
        """
        # Tokenizza per BM25
        tokenized_chunks = [chunk.lower().split() for chunk in context_chunks]
        tokenized_question = question.lower().split()
        
        # Crea BM25
        bm25 = BM25Okapi(tokenized_chunks)
        
        # Score
        scores = bm25.get_scores(tokenized_question)
        
        # Top-k
        top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k]
        
        retrieved = [context_chunks[i] for i in top_indices]
        return retrieved
    
    def prepare_context(self, question: str, context_chunks: List[str], k: int) -> str:
        """Prepara contesto per reasoner"""
        retrieved = self.retrieve_chunks(question, context_chunks, k)
        context = "\n\n".join(retrieved)
        return context


=== END OF FILE src/models/rag_baseline.py ===


=== START OF FILE src/models/summarizer_baseline.py ===
Percorso: src/models/summarizer_baseline.py
Dimensione: 1758 bytes
Ultima modifica: 2025-12-31 10:31:58.112614013 +0000

"""
Baseline: summary per chunk usando teacher
"""
import json
from pathlib import Path
from typing import List
import logging

logger = logging.getLogger(__name__)


class SummarizerBaseline:
    """Baseline che riassume ogni chunk con teacher"""
    
    def __init__(self, config: dict, teacher):
        self.config = config
        self.teacher = teacher
        self.summary_tokens = config["baselines"]["summary_tokens_per_chunk"]
        self.cache_dir = Path(config["paths"]["cache_dir"]) / "summaries"
        self.cache_dir.mkdir(parents=True, exist_ok=True)
    
    def summarize_chunk(self, chunk_text: str, chunk_id: int) -> str:
        """Riassume un chunk"""
        # Controlla cache
        cache_file = self.cache_dir / f"summary_{hash(chunk_text)}.txt"
        if cache_file.exists():
            return cache_file.read_text().strip()
        
        # Prompt per summary
        prompt = f"Summarize the following text in at most {self.summary_tokens} tokens:\n\n{chunk_text}\n\nSummary:"
        
        # Usa teacher per generare summary
        summary = self.teacher.generate_answer_vllm(
            chunk_text,
            f"Summarize this text in at most {self.summary_tokens} tokens.",
            max_tokens=self.summary_tokens,
            cache_dir=self.cache_dir / "vllm_cache"
        )
        
        # Salva in cache
        cache_file.write_text(summary)
        
        return summary
    
    def prepare_context(self, context_chunks: List[str]) -> str:
        """Prepara contesto con summaries"""
        summaries = []
        for i, chunk in enumerate(context_chunks):
            summary = self.summarize_chunk(chunk, i)
            summaries.append(summary)
        
        return "\n\n".join(summaries)


=== END OF FILE src/models/summarizer_baseline.py ===


=== START OF FILE src/models/pooling_baseline.py ===
Percorso: src/models/pooling_baseline.py
Dimensione: 2895 bytes
Ultima modifica: 2025-12-31 10:31:58.112614013 +0000

"""
Baseline: pooling semplice (mean/max) di hidden states teacher
"""
import torch
import torch.nn as nn
from typing import List, Optional
import logging

logger = logging.getLogger(__name__)


class PoolingBaseline:
    """Baseline con pooling di hidden states"""
    
    def __init__(self, config: dict, teacher):
        self.config = config
        self.teacher = teacher
        self.n_latents = config["n_latents"]
        self.d_lat = config["d_lat"]
        self.pooling_type = "mean"  # o "max"
        
        # Proiezione da d_teacher a d_lat
        self.proj = nn.Linear(config.get("d_teacher", 4096), self.d_lat)
        # Sposta su device quando necessario (lazy)
        self._device = None
    
    def pool_chunk(self, teacher_states: torch.Tensor) -> torch.Tensor:
        """
        Pooling di hidden states a N_lat vectors
        
        Args:
            teacher_states: [seq_len, d_teacher]
        
        Returns:
            [N_lat, d_lat]
        """
        # Sposta proj su device se necessario
        device = teacher_states.device
        if self._device != device:
            self.proj = self.proj.to(device)
            self._device = device
        
        seq_len = teacher_states.shape[0]
        
        if self.pooling_type == "mean":
            # Mean pooling
            pooled = teacher_states.mean(dim=0, keepdim=True)  # [1, d_teacher]
        else:
            # Max pooling
            pooled = teacher_states.max(dim=0)[0].unsqueeze(0)  # [1, d_teacher]
        
        # Proietta a d_lat
        pooled = self.proj(pooled)  # [1, d_lat]
        
        # Ripeti per avere N_lat vectors
        latents = pooled.repeat(self.n_latents, 1)  # [N_lat, d_lat]
        
        return latents
    
    def get_latents(self, context_chunks: List[str], device: torch.device) -> torch.Tensor:
        """
        Ottiene latents per tutti i chunk
        
        Returns:
            [num_chunks * N_lat, d_lat]
        """
        all_latents = []
        layer_indices = self.config.get("layer_indices", [-6])
        
        for chunk_text in context_chunks:
            if self.teacher.hidden_states_available:
                states = self.teacher.get_hidden_states(chunk_text, layer_indices)
                if states is not None:
                    states = states.to(device)
                    latents = self.pool_chunk(states)
                    all_latents.append(latents)
                else:
                    # Fallback: latents dummy
                    latents = torch.randn(self.n_latents, self.d_lat, device=device) * 0.01
                    all_latents.append(latents)
            else:
                # Fallback: latents dummy
                latents = torch.randn(self.n_latents, self.d_lat, device=device) * 0.01
                all_latents.append(latents)
        
        return torch.cat(all_latents, dim=0)


=== END OF FILE src/models/pooling_baseline.py ===


=== START OF FILE src/train/__init__.py ===
Percorso: src/train/__init__.py
Dimensione: 20 bytes
Ultima modifica: 2025-12-31 10:31:58.112614013 +0000

# Training modules


=== END OF FILE src/train/__init__.py ===


=== START OF FILE src/train/train_compressor.py ===
Percorso: src/train/train_compressor.py
Dimensione: 12335 bytes
Ultima modifica: 2025-12-31 10:31:58.112614013 +0000

"""
Training loop per compressore con task distillation
"""
import argparse
import yaml
from pathlib import Path
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from tqdm import tqdm
import json

from src.utils.seed import set_seed
from src.utils.logging import setup_logging, get_logger
from src.utils.tokenization import get_tokenizer
from src.utils.timers import Timer
from src.utils.vram import get_vram_usage, reset_peak_memory
from src.data.dataset import LatentCompressionDataset
from src.models.teacher_reader import TeacherReader
from src.models.compressor import LatentCompressor
from src.models.reasoner_wrapper import ReasonerWrapper


def load_config(config_path: str, exp_name: str = None) -> dict:
    """Carica configurazione base e eventuale override esperimento"""
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    
    # Se exp_name specificato, carica override da exp_grid.yaml
    if exp_name:
        exp_grid_path = Path(config_path).parent / "exp_grid.yaml"
        if exp_grid_path.exists():
            with open(exp_grid_path, 'r') as f:
                exp_grid = yaml.safe_load(f)
            if exp_name in exp_grid.get("experiments", {}):
                exp_config = exp_grid["experiments"][exp_name]
                # Merge: exp_config override su config base
                config.update(exp_config)
                # Merge nested dicts
                if "train" in exp_config:
                    config["train"].update(exp_config["train"])
    
    return config


def cache_teacher_answers(dataset, teacher, cache_dir: Path):
    """Pre-calcola risposte teacher per training"""
    cache_file = cache_dir / "teacher_answers.jsonl"
    
    if cache_file.exists():
        logger.info(f"Caricamento risposte teacher da cache: {cache_file}")
        answers = []
        with open(cache_file, 'r') as f:
            for line in f:
                answers.append(json.loads(line)["answer"])
        return answers
    
    logger.info("Generazione risposte teacher (questo pu√≤ richiedere tempo)...")
    answers = []
    cache_dir.mkdir(parents=True, exist_ok=True)
    
    with open(cache_file, 'w') as f:
        for i, example in enumerate(tqdm(dataset.examples)):
            answer = teacher.generate_answer_vllm(
                example["context"],
                example["question"],
                cache_dir=cache_dir / "vllm_cache"
            )
            answers.append(answer)
            f.write(json.dumps({"answer": answer}) + '\n')
    
    logger.info(f"Salvate {len(answers)} risposte teacher")
    return answers


def train_step(compressor, reasoner, teacher, batch, teacher_answers, 
               device, config, optimizer):
    """Un passo di training"""
    compressor.train()
    
    # Estrai hidden states teacher per ogni chunk
    teacher_states_list = []
    layer_indices = config.get("layer_indices", [-6])
    
    # Gestisci batch: context_chunks pu√≤ essere lista di liste o lista singola
    context_chunks = batch["context_chunks"]
    if isinstance(context_chunks[0], list):
        # Batch size > 1: prendi primo esempio
        context_chunks = context_chunks[0]
    
    for chunk_text in context_chunks:
        if teacher.hidden_states_available:
            states = teacher.get_hidden_states(chunk_text, layer_indices)
            if states is not None:
                teacher_states_list.append(states.to(device))
            else:
                # Fallback: usa embeddings del tokenizer come proxy
                # (non ideale ma funziona per training logits-only)
                tokens = teacher.tokenizer.encode(chunk_text, return_tensors="pt")
                # Crea embeddings dummy (dovremmo usare hidden_dim del teacher)
                d_teacher = config.get("d_teacher", 4096)
                dummy_states = torch.randn(len(tokens[0]), d_teacher, device=device) * 0.01
                teacher_states_list.append(dummy_states)
        else:
            # Fallback: embeddings dummy
            d_teacher = config.get("d_teacher", 4096)
            tokens = teacher.tokenizer.encode(chunk_text, return_tensors="pt")
            dummy_states = torch.randn(len(tokens[0]), d_teacher, device=device) * 0.01
            teacher_states_list.append(dummy_states)
    
    # Compressore produce latents
    latents = compressor(teacher_states_list)  # [total_latents, d_lat]
    
    # Reasoner forward con latents
    question = batch["question"][0] if isinstance(batch["question"], list) else batch["question"]
    # Ottieni indice esempio dal batch
    if isinstance(batch.get("_idx"), list):
        answer_idx = batch["_idx"][0]
    elif isinstance(batch.get("_idx"), torch.Tensor):
        answer_idx = batch["_idx"].item()
    else:
        answer_idx = 0
    
    # Usa teacher answer se disponibile, altrimenti ground truth
    if answer_idx < len(teacher_answers):
        target_answer = teacher_answers[answer_idx]
    else:
        target_answer = batch["answer"][0] if isinstance(batch["answer"], list) else batch["answer"]
    
    result = reasoner.forward_with_latents(
        latents,
        question,
        target_answer
    )
    
    loss = result["loss"]
    
    # Backward
    optimizer.zero_grad()
    loss.backward()
    torch.nn.utils.clip_grad_norm_(compressor.parameters(), max_norm=1.0)
    optimizer.step()
    
    return loss.item()


def validate(compressor, reasoner, teacher, val_dataset, device, config):
    """Valutazione su validation set"""
    compressor.eval()
    
    losses = []
    predictions = []
    references = []
    
    with torch.no_grad():
        for i in range(min(50, len(val_dataset))):  # Limita a 50 per velocit√†
            example = val_dataset[i]
            
            # Estrai hidden states
            teacher_states_list = []
            for chunk_text in example["context_chunks"]:
                if teacher.hidden_states_available:
                    states = teacher.get_hidden_states(chunk_text, config.get("layer_indices", [-6]))
                    if states is not None:
                        teacher_states_list.append(states.to(device))
                    else:
                        d_teacher = config.get("d_teacher", 4096)
                        dummy = torch.randn(100, d_teacher, device=device) * 0.01
                        teacher_states_list.append(dummy)
                else:
                    d_teacher = config.get("d_teacher", 4096)
                    dummy = torch.randn(100, d_teacher, device=device) * 0.01
                    teacher_states_list.append(dummy)
            
            # Compressore
            latents = compressor(teacher_states_list)
            
            # Genera risposta
            question = example["question"]
            pred_answer = reasoner.generate_with_latents(
                latents,
                question,
                max_new_tokens=config["eval"]["max_new_tokens"],
                temperature=config["eval"]["temperature"]
            )
            
            predictions.append(pred_answer)
            references.append(example["answer"])
    
    # Calcola metriche
    from src.utils.metrics import compute_metrics
    metrics = compute_metrics(predictions, references)
    
    return metrics


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=str, default="configs/base.yaml")
    parser.add_argument("--exp", type=str, default="S0")
    parser.add_argument("--resume", type=str, default=None)
    args = parser.parse_args()
    
    # Setup
    config = load_config(args.config, args.exp)
    set_seed(42)
    logger = setup_logging(config)
    logger.info(f"Esperimento: {args.exp}")
    logger.info(f"Config: {config}")
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"Device: {device}")
    
    # Paths
    exp_dir = Path(config["train"]["checkpoint_dir"]) / args.exp
    exp_dir.mkdir(parents=True, exist_ok=True)
    cache_dir = Path(config["paths"]["cache_dir"])
    cache_dir.mkdir(parents=True, exist_ok=True)
    
    # Carica modelli
    logger.info("Caricamento modelli...")
    teacher = TeacherReader(config)
    reasoner = ReasonerWrapper(config)
    compressor = LatentCompressor(config).to(device)
    
    logger.info(f"Hidden states disponibili: {teacher.hidden_states_available}")
    
    # Carica dataset
    data_dir = Path(config["paths"]["data_dir"])
    train_file = data_dir / "raw" / "synthetic_train.jsonl"
    val_file = data_dir / "raw" / "synthetic_val.jsonl"
    
    teacher_tokenizer = get_tokenizer(config["teacher_model"])
    train_dataset = LatentCompressionDataset(
        str(train_file),
        teacher_tokenizer,
        config["chunk_size_tokens"],
        config["chunk_overlap_tokens"]
    )
    val_dataset = LatentCompressionDataset(
        str(val_file),
        teacher_tokenizer,
        config["chunk_size_tokens"],
        config["chunk_overlap_tokens"],
        max_examples=200
    )
    
    # Cache teacher answers
    teacher_answers = cache_teacher_answers(train_dataset, teacher, cache_dir)
    
    # DataLoader
    train_loader = DataLoader(
        train_dataset,
        batch_size=config["train"]["batch_size"],
        shuffle=True,
        num_workers=0
    )
    
    # Optimizer
    optimizer = torch.optim.AdamW(
        compressor.parameters(),
        lr=config["train"]["lr"]
    )
    
    # Training loop
    train_config = config["train"]
    max_steps = train_config["max_steps"]
    eval_steps = train_config.get("eval_steps", 200)
    save_steps = train_config.get("save_steps", 500)
    grad_accum = train_config.get("gradient_accumulation_steps", 8)
    
    logger.info(f"Inizio training: {max_steps} steps")
    timer = Timer()
    reset_peak_memory()
    
    step = 0
    best_val_f1 = 0.0
    
    for epoch in range(100):  # Max epochs
        for batch in train_loader:
            if step >= max_steps:
                break
            
            # Training step
            with timer.time("train_step"):
                loss = train_step(
                    compressor, reasoner, teacher, batch, teacher_answers,
                    device, config, optimizer
                )
            
            step += 1
            
            if step % 10 == 0:
                logger.info(f"Step {step}/{max_steps}, Loss: {loss:.4f}")
                vram = get_vram_usage()
                logger.info(f"VRAM: {vram['allocated']:.2f}GB / {vram['max_allocated']:.2f}GB")
            
            # Validation
            if step % eval_steps == 0:
                logger.info("Validazione...")
                val_metrics = validate(compressor, reasoner, teacher, val_dataset, device, config)
                logger.info(f"Val metrics: {val_metrics}")
                
                if val_metrics["token_f1"] > best_val_f1:
                    best_val_f1 = val_metrics["token_f1"]
                    # Salva best model
                    checkpoint = {
                        "step": step,
                        "compressor_state": compressor.state_dict(),
                        "optimizer_state": optimizer.state_dict(),
                        "val_metrics": val_metrics
                    }
                    torch.save(checkpoint, exp_dir / "best_model.pt")
                    logger.info(f"Salvato best model (F1: {best_val_f1:.4f})")
            
            # Save checkpoint
            if step % save_steps == 0:
                checkpoint = {
                    "step": step,
                    "compressor_state": compressor.state_dict(),
                    "optimizer_state": optimizer.state_dict()
                }
                torch.save(checkpoint, exp_dir / f"checkpoint_step_{step}.pt")
                logger.info(f"Checkpoint salvato: step {step}")
        
        if step >= max_steps:
            break
    
    logger.info("Training completato!")
    logger.info(f"Best val F1: {best_val_f1:.4f}")
    
    # Salva modello finale
    final_checkpoint = {
        "step": step,
        "compressor_state": compressor.state_dict(),
        "optimizer_state": optimizer.state_dict()
    }
    torch.save(final_checkpoint, exp_dir / "final_model.pt")


if __name__ == "__main__":
    main()


=== END OF FILE src/train/train_compressor.py ===


=== START OF FILE src/train/distill.py ===
Percorso: src/train/distill.py
Dimensione: 200 bytes
Ultima modifica: 2025-12-31 10:31:58.112614013 +0000

"""
Utilities per distillation (opzionale, gi√† integrato in train_compressor)
"""
# La distillation √® gi√† implementata in train_compressor.py
# Questo file √® un placeholder per future estensioni


=== END OF FILE src/train/distill.py ===


=== START OF FILE src/eval/__init__.py ===
Percorso: src/eval/__init__.py
Dimensione: 22 bytes
Ultima modifica: 2025-12-31 10:31:58.112614013 +0000

# Evaluation modules


=== END OF FILE src/eval/__init__.py ===


=== START OF FILE src/eval/run_eval.py ===
Percorso: src/eval/run_eval.py
Dimensione: 12921 bytes
Ultima modifica: 2025-12-31 10:31:58.112614013 +0000

"""
Sistema di valutazione completo
"""
import argparse
import yaml
import json
from pathlib import Path
import torch
from tqdm import tqdm

from src.utils.seed import set_seed
from src.utils.logging import setup_logging, get_logger
from src.utils.tokenization import get_tokenizer
from src.utils.timers import Timer
from src.utils.vram import get_vram_usage, reset_peak_memory
from src.utils.metrics import compute_metrics, exact_match, token_f1
from src.data.dataset import LatentCompressionDataset
from src.models.teacher_reader import TeacherReader
from src.models.compressor import LatentCompressor
from src.models.reasoner_wrapper import ReasonerWrapper
from src.models.rag_baseline import RAGBaseline
from src.models.summarizer_baseline import SummarizerBaseline
from src.models.pooling_baseline import PoolingBaseline


def load_config(config_path: str, exp_name: str = None) -> dict:
    """Carica configurazione"""
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    
    if exp_name:
        exp_grid_path = Path(config_path).parent / "exp_grid.yaml"
        if exp_grid_path.exists():
            with open(exp_grid_path, 'r') as f:
                exp_grid = yaml.safe_load(f)
            if exp_name in exp_grid.get("experiments", {}):
                exp_config = exp_grid["experiments"][exp_name]
                config.update(exp_config)
                if "train" in exp_config:
                    config["train"].update(exp_config["train"])
    
    return config


def evaluate_teacher(teacher, dataset, config):
    """Valuta teacher full context"""
    logger = get_logger(__name__)
    logger.info("Valutazione Teacher Full Context...")
    
    predictions = []
    references = []
    latencies = []
    timer = Timer()
    
    for i, example in enumerate(tqdm(dataset)):
        with timer.time("teacher_generate"):
            pred = teacher.generate_answer_vllm(
                example["context"],
                example["question"],
                max_tokens=config["eval"]["max_new_tokens"],
                temperature=config["eval"]["temperature"]
            )
        
        predictions.append(pred)
        references.append(example["answer"])
        latencies.append(timer.get_mean("teacher_generate"))
    
    metrics = compute_metrics(predictions, references)
    metrics["avg_latency"] = sum(latencies) / len(latencies) if latencies else 0.0
    metrics["method"] = "teacher_full"
    
    return metrics


def evaluate_student(compressor, reasoner, teacher, dataset, config, device):
    """Valuta student (compressor + reasoner)"""
    logger = get_logger(__name__)
    logger.info("Valutazione Student (Compressor + Reasoner)...")
    
    predictions = []
    references = []
    latencies = []
    timer = Timer()
    
    compressor.eval()
    
    for example in tqdm(dataset):
        with timer.time("student_generate"):
            # Estrai hidden states
            teacher_states_list = []
            layer_indices = config.get("layer_indices", [-6])
            
            for chunk_text in example["context_chunks"]:
                if teacher.hidden_states_available:
                    states = teacher.get_hidden_states(chunk_text, layer_indices)
                    if states is not None:
                        teacher_states_list.append(states.to(device))
                    else:
                        d_teacher = config.get("d_teacher", 4096)
                        dummy = torch.randn(100, d_teacher, device=device) * 0.01
                        teacher_states_list.append(dummy)
                else:
                    d_teacher = config.get("d_teacher", 4096)
                    dummy = torch.randn(100, d_teacher, device=device) * 0.01
                    teacher_states_list.append(dummy)
            
            # Compressore
            with torch.no_grad():
                latents = compressor(teacher_states_list)
            
            # Reasoner
            pred = reasoner.generate_with_latents(
                latents,
                example["question"],
                max_new_tokens=config["eval"]["max_new_tokens"],
                temperature=config["eval"]["temperature"]
            )
        
        predictions.append(pred)
        references.append(example["answer"])
        latencies.append(timer.get_mean("student_generate"))
    
    metrics = compute_metrics(predictions, references)
    metrics["avg_latency"] = sum(latencies) / len(latencies) if latencies else 0.0
    metrics["method"] = "student"
    
    return metrics


def evaluate_rag(rag_baseline, reasoner, dataset, config, device):
    """Valuta baseline RAG"""
    logger = get_logger(__name__)
    
    all_metrics = []
    
    for k in config["baselines"]["rag_k"]:
        logger.info(f"Valutazione RAG k={k}...")
        predictions = []
        references = []
        timer = Timer()
        
        for example in tqdm(dataset):
            with timer.time("rag_generate"):
                # Retrieval
                context = rag_baseline.prepare_context(
                    example["question"],
                    example["context_chunks"],
                    k
                )
                
                # Reasoner con contesto recuperato
                # Usa reasoner normalmente (non con latents)
                inputs = reasoner.tokenizer(
                    f"{context}\n\nQuestion: {example['question']}\nAnswer:",
                    return_tensors="pt",
                    truncation=True,
                    max_length=reasoner.max_ctx
                ).to(device)
                
                with torch.no_grad():
                    outputs = reasoner.model.generate(
                        **inputs,
                        max_new_tokens=config["eval"]["max_new_tokens"],
                        temperature=config["eval"]["temperature"] if config["eval"]["temperature"] > 0 else None,
                        do_sample=config["eval"]["temperature"] > 0,
                        pad_token_id=reasoner.tokenizer.eos_token_id
                    )
                
                pred = reasoner.tokenizer.decode(
                    outputs[0][inputs["input_ids"].shape[1]:],
                    skip_special_tokens=True
                )
            
            predictions.append(pred)
            references.append(example["answer"])
        
        metrics = compute_metrics(predictions, references)
        metrics["avg_latency"] = timer.get_mean("rag_generate")
        metrics["method"] = f"rag_k{k}"
        all_metrics.append(metrics)
    
    return all_metrics


def evaluate_summary(summarizer, reasoner, dataset, config, device):
    """Valuta baseline summary"""
    logger = get_logger(__name__)
    logger.info("Valutazione Summary Baseline...")
    
    predictions = []
    references = []
    timer = Timer()
    
    for example in tqdm(dataset):
        with timer.time("summary_generate"):
            # Summary
            context = summarizer.prepare_context(example["context_chunks"])
            
            # Reasoner
            inputs = reasoner.tokenizer(
                f"{context}\n\nQuestion: {example['question']}\nAnswer:",
                return_tensors="pt",
                truncation=True,
                max_length=reasoner.max_ctx
            ).to(device)
            
            with torch.no_grad():
                outputs = reasoner.model.generate(
                    **inputs,
                    max_new_tokens=config["eval"]["max_new_tokens"],
                    temperature=config["eval"]["temperature"] if config["eval"]["temperature"] > 0 else None,
                    do_sample=config["eval"]["temperature"] > 0,
                    pad_token_id=reasoner.tokenizer.eos_token_id
                )
            
            pred = reasoner.tokenizer.decode(
                outputs[0][inputs["input_ids"].shape[1]:],
                skip_special_tokens=True
            )
        
        predictions.append(pred)
        references.append(example["answer"])
    
    metrics = compute_metrics(predictions, references)
    metrics["avg_latency"] = timer.get_mean("summary_generate")
    metrics["method"] = "summary"
    
    return metrics


def evaluate_pooling(pooling, reasoner, teacher, dataset, config, device):
    """Valuta baseline pooling"""
    logger = get_logger(__name__)
    logger.info("Valutazione Pooling Baseline...")
    
    predictions = []
    references = []
    timer = Timer()
    
    for example in tqdm(dataset):
        with timer.time("pooling_generate"):
            # Pooling latents
            latents = pooling.get_latents(example["context_chunks"], device)
            
            # Reasoner con latents
            pred = reasoner.generate_with_latents(
                latents,
                example["question"],
                max_new_tokens=config["eval"]["max_new_tokens"],
                temperature=config["eval"]["temperature"]
            )
        
        predictions.append(pred)
        references.append(example["answer"])
    
    metrics = compute_metrics(predictions, references)
    metrics["avg_latency"] = timer.get_mean("pooling_generate")
    metrics["method"] = "pooling"
    
    return metrics


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=str, default="configs/base.yaml")
    parser.add_argument("--exp", type=str, default="S0")
    parser.add_argument("--checkpoint", type=str, default=None)
    args = parser.parse_args()
    
    # Setup
    config = load_config(args.config, args.exp)
    set_seed(42)
    logger = setup_logging(config)
    logger.info(f"Esperimento: {args.exp}")
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"Device: {device}")
    
    # Paths
    output_dir = Path(config["paths"]["output_dir"])
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Carica modelli
    logger.info("Caricamento modelli...")
    teacher = TeacherReader(config)
    reasoner = ReasonerWrapper(config)
    
    # Carica compressore se checkpoint specificato
    compressor = None
    if args.checkpoint:
        checkpoint_path = Path(args.checkpoint)
        if checkpoint_path.exists():
            compressor = LatentCompressor(config).to(device)
            checkpoint = torch.load(checkpoint_path, map_location=device)
            compressor.load_state_dict(checkpoint["compressor_state"])
            logger.info(f"Compressore caricato da {checkpoint_path}")
        else:
            logger.warning(f"Checkpoint non trovato: {checkpoint_path}")
    
    # Carica dataset test
    data_dir = Path(config["paths"]["data_dir"])
    test_file = data_dir / "raw" / "synthetic_test.jsonl"
    
    teacher_tokenizer = get_tokenizer(config["teacher_model"])
    test_dataset = LatentCompressionDataset(
        str(test_file),
        teacher_tokenizer,
        config["chunk_size_tokens"],
        config["chunk_overlap_tokens"]
    )
    
    logger.info(f"Dataset test: {len(test_dataset)} esempi")
    
    # Valutazioni
    all_results = []
    
    reset_peak_memory()
    
    # 1. Teacher full
    teacher_metrics = evaluate_teacher(teacher, test_dataset, config)
    all_results.append(teacher_metrics)
    logger.info(f"Teacher: {teacher_metrics}")
    
    # 2. Student (se disponibile)
    if compressor is not None:
        student_metrics = evaluate_student(
            compressor, reasoner, teacher, test_dataset, config, device
        )
        all_results.append(student_metrics)
        logger.info(f"Student: {student_metrics}")
    
    # 3. RAG
    rag_baseline = RAGBaseline(config)
    rag_metrics_list = evaluate_rag(rag_baseline, reasoner, test_dataset, config, device)
    all_results.extend(rag_metrics_list)
    for m in rag_metrics_list:
        logger.info(f"RAG {m['method']}: {m}")
    
    # 4. Summary
    summarizer = SummarizerBaseline(config, teacher)
    summary_metrics = evaluate_summary(summarizer, reasoner, test_dataset, config, device)
    all_results.append(summary_metrics)
    logger.info(f"Summary: {summary_metrics}")
    
    # 5. Pooling
    pooling = PoolingBaseline(config, teacher)
    pooling_metrics = evaluate_pooling(
        pooling, reasoner, teacher, test_dataset, config, device
    )
    all_results.append(pooling_metrics)
    logger.info(f"Pooling: {pooling_metrics}")
    
    # VRAM stats
    vram = get_vram_usage()
    logger.info(f"VRAM peak: {vram['max_allocated']:.2f}GB")
    
    # Salva risultati
    results_file = output_dir / "report.json"
    with open(results_file, 'w') as f:
        json.dump({
            "experiment": args.exp,
            "results": all_results,
            "vram": vram
        }, f, indent=2)
    
    logger.info(f"Risultati salvati in {results_file}")
    
    return results_file


if __name__ == "__main__":
    main()


=== END OF FILE src/eval/run_eval.py ===


=== START OF FILE src/eval/report.py ===
Percorso: src/eval/report.py
Dimensione: 3870 bytes
Ultima modifica: 2025-12-31 10:31:58.112614013 +0000

"""
Genera report markdown e plot
"""
import argparse
import json
from pathlib import Path
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np


def load_results(results_file: str) -> dict:
    """Carica risultati da JSON"""
    with open(results_file, 'r') as f:
        return json.load(f)


def generate_markdown_report(results: dict, output_file: Path):
    """Genera report markdown"""
    lines = []
    lines.append("# Report Valutazione Compressione Latente\n")
    lines.append(f"Esperimento: {results['experiment']}\n")
    
    # Tabella risultati
    lines.append("## Risultati\n")
    lines.append("| Metodo | Exact Match | Token F1 | Latency (s) |\n")
    lines.append("|--------|-------------|----------|-------------|\n")
    
    for result in results["results"]:
        method = result.get("method", "unknown")
        em = result.get("exact_match", 0.0)
        f1 = result.get("token_f1", 0.0)
        latency = result.get("avg_latency", 0.0)
        lines.append(f"| {method} | {em:.4f} | {f1:.4f} | {latency:.4f} |\n")
    
    # VRAM
    lines.append("\n## VRAM Usage\n")
    vram = results.get("vram", {})
    lines.append(f"- Allocated: {vram.get('allocated', 0):.2f} GB\n")
    lines.append(f"- Peak: {vram.get('max_allocated', 0):.2f} GB\n")
    
    # Conclusioni
    lines.append("\n## Conclusioni\n")
    lines.append("Analisi dei risultati:\n")
    
    # Trova best F1
    best_f1 = max(r.get("token_f1", 0) for r in results["results"])
    best_method = next(r["method"] for r in results["results"] if r.get("token_f1", 0) == best_f1)
    lines.append(f"- Miglior F1: {best_method} ({best_f1:.4f})\n")
    
    # Trova fastest
    fastest = min(r.get("avg_latency", float('inf')) for r in results["results"])
    fastest_method = next(r["method"] for r in results["results"] if r.get("avg_latency", float('inf')) == fastest)
    lines.append(f"- Pi√π veloce: {fastest_method} ({fastest:.4f}s)\n")
    
    # Scrivi file
    output_file.write_text("".join(lines))
    print(f"Report markdown salvato in {output_file}")


def generate_plots(results: dict, output_dir: Path):
    """Genera plot"""
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Prepara dati
    methods = [r["method"] for r in results["results"]]
    f1_scores = [r.get("token_f1", 0) for r in results["results"]]
    latencies = [r.get("avg_latency", 0) for r in results["results"]]
    
    # Plot 1: F1 vs Latency (Pareto)
    plt.figure(figsize=(10, 6))
    plt.scatter(latencies, f1_scores, s=100, alpha=0.6)
    for i, method in enumerate(methods):
        plt.annotate(method, (latencies[i], f1_scores[i]), fontsize=8)
    plt.xlabel("Latency (s)")
    plt.ylabel("Token F1")
    plt.title("Pareto Plot: Quality vs Latency")
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_dir / "pareto_latency.png", dpi=150)
    plt.close()
    
    # Plot 2: Bar chart F1
    plt.figure(figsize=(12, 6))
    plt.bar(methods, f1_scores)
    plt.xlabel("Metodo")
    plt.ylabel("Token F1")
    plt.title("Confronto Token F1")
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.savefig(output_dir / "f1_comparison.png", dpi=150)
    plt.close()
    
    print(f"Plot salvati in {output_dir}")


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--input", type=str, default="outputs/report.json")
    parser.add_argument("--output", type=str, default="outputs/report.md")
    args = parser.parse_args()
    
    # Carica risultati
    results = load_results(args.input)
    
    # Genera report
    output_file = Path(args.output)
    generate_markdown_report(results, output_file)
    
    # Genera plot
    plot_dir = output_file.parent / "plots"
    generate_plots(results, plot_dir)
    
    print("Report generato con successo!")


if __name__ == "__main__":
    main()


=== END OF FILE src/eval/report.py ===


=== START OF FILE src/utils/__init__.py ===
Percorso: src/utils/__init__.py
Dimensione: 13 bytes
Ultima modifica: 2025-12-31 10:31:58.093613917 +0000

# Utilities


=== END OF FILE src/utils/__init__.py ===


=== START OF FILE src/utils/logging.py ===
Percorso: src/utils/logging.py
Dimensione: 1167 bytes
Ultima modifica: 2025-12-31 10:31:58.093613917 +0000

"""
Sistema di logging standardizzato
"""
import logging
import os
from datetime import datetime
from pathlib import Path


def setup_logging(config=None, log_dir="logs"):
    """
    Configura il sistema di logging
    
    Args:
        config: Config object con LOG_LEVEL e LOG_DOMINI_SEPARATI
        log_dir: Directory per i log
    """
    log_dir_path = Path(log_dir)
    log_dir_path.mkdir(parents=True, exist_ok=True)
    
    # Nome file con timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = log_dir_path / f"diario_{timestamp}.log"
    
    # Configurazione logging
    log_level = getattr(config, 'LOG_LEVEL', logging.INFO) if config else logging.INFO
    
    logging.basicConfig(
        level=log_level,
        format='[%(asctime)s] [%(name)s] [%(levelname)s] %(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler()
        ]
    )
    
    logger = logging.getLogger(__name__)
    logger.info(f"Logging inizializzato. File: {log_file}")
    return logger


def get_logger(name):
    """Ottiene un logger con nome specifico"""
    return logging.getLogger(name)


=== END OF FILE src/utils/logging.py ===


=== START OF FILE src/utils/metrics.py ===
Percorso: src/utils/metrics.py
Dimensione: 1584 bytes
Ultima modifica: 2025-12-31 10:31:58.093613917 +0000

"""
Metriche di valutazione
"""
import re
from typing import List, Dict
import numpy as np


def normalize_answer(text: str) -> str:
    """Normalizza risposta per confronto"""
    text = text.lower().strip()
    # Rimuovi punteggiatura extra
    text = re.sub(r'[^\w\s]', '', text)
    # Rimuovi spazi multipli
    text = re.sub(r'\s+', ' ', text)
    return text


def exact_match(pred: str, gold: str) -> bool:
    """Exact match normalizzato"""
    return normalize_answer(pred) == normalize_answer(gold)


def token_f1(pred: str, gold: str) -> float:
    """F1 a livello di token"""
    pred_tokens = set(normalize_answer(pred).split())
    gold_tokens = set(normalize_answer(gold).split())
    
    if len(gold_tokens) == 0:
        return 1.0 if len(pred_tokens) == 0 else 0.0
    
    if len(pred_tokens) == 0:
        return 0.0
    
    intersection = pred_tokens & gold_tokens
    precision = len(intersection) / len(pred_tokens) if pred_tokens else 0.0
    recall = len(intersection) / len(gold_tokens) if gold_tokens else 0.0
    
    if precision + recall == 0:
        return 0.0
    
    return 2 * precision * recall / (precision + recall)


def compute_metrics(predictions: List[str], references: List[str]) -> Dict[str, float]:
    """Calcola metriche aggregate"""
    em_scores = [exact_match(p, r) for p, r in zip(predictions, references)]
    f1_scores = [token_f1(p, r) for p, r in zip(predictions, references)]
    
    return {
        "exact_match": np.mean(em_scores),
        "token_f1": np.mean(f1_scores),
        "num_examples": len(predictions)
    }


=== END OF FILE src/utils/metrics.py ===


=== START OF FILE src/utils/tokenization.py ===
Percorso: src/utils/tokenization.py
Dimensione: 1103 bytes
Ultima modifica: 2025-12-31 10:31:58.093613917 +0000

"""
Utilities per tokenizzazione
"""
from transformers import AutoTokenizer
from typing import List, Tuple


def get_tokenizer(model_name: str):
    """Carica tokenizer per modello"""
    return AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)


def count_tokens(text: str, tokenizer) -> int:
    """Conta token in un testo"""
    return len(tokenizer.encode(text, add_special_tokens=False))


def chunk_text(text: str, tokenizer, chunk_size: int, overlap: int = 0) -> List[Tuple[str, int, int]]:
    """
    Divide testo in chunk con overlap
    
    Returns:
        List of (chunk_text, start_pos, end_pos) in tokens
    """
    tokens = tokenizer.encode(text, add_special_tokens=False)
    chunks = []
    
    start = 0
    while start < len(tokens):
        end = min(start + chunk_size, len(tokens))
        chunk_tokens = tokens[start:end]
        chunk_text = tokenizer.decode(chunk_tokens, skip_special_tokens=True)
        chunks.append((chunk_text, start, end))
        
        if end >= len(tokens):
            break
        start = end - overlap
    
    return chunks


=== END OF FILE src/utils/tokenization.py ===


=== START OF FILE src/utils/timers.py ===
Percorso: src/utils/timers.py
Dimensione: 1580 bytes
Ultima modifica: 2025-12-31 10:31:58.093613917 +0000

"""
Utilities per timing e profiling
"""
import time
from contextlib import contextmanager
from typing import Dict
import torch


class Timer:
    """Timer per misurare tempi di esecuzione"""
    
    def __init__(self):
        self.times: Dict[str, float] = {}
        self.starts: Dict[str, float] = {}
    
    @contextmanager
    def time(self, name: str):
        """Context manager per timing"""
        self.start(name)
        try:
            yield
        finally:
            self.stop(name)
    
    def start(self, name: str):
        """Avvia timer"""
        if torch.cuda.is_available():
            torch.cuda.synchronize()
        self.starts[name] = time.time()
    
    def stop(self, name: str):
        """Ferma timer e registra"""
        if torch.cuda.is_available():
            torch.cuda.synchronize()
        if name in self.starts:
            elapsed = time.time() - self.starts[name]
            if name not in self.times:
                self.times[name] = []
            self.times[name].append(elapsed)
            del self.starts[name]
    
    def get_mean(self, name: str) -> float:
        """Ottiene tempo medio per nome"""
        if name in self.times:
            return sum(self.times[name]) / len(self.times[name])
        return 0.0
    
    def get_total(self, name: str) -> float:
        """Ottiene tempo totale per nome"""
        if name in self.times:
            return sum(self.times[name])
        return 0.0
    
    def reset(self):
        """Reset tutti i timer"""
        self.times.clear()
        self.starts.clear()


=== END OF FILE src/utils/timers.py ===


=== START OF FILE src/utils/seed.py ===
Percorso: src/utils/seed.py
Dimensione: 412 bytes
Ultima modifica: 2025-12-31 10:31:58.093613917 +0000

"""
Utilities per seeding riproducibile
"""
import random
import numpy as np
import torch


def set_seed(seed=42):
    """
    Imposta seed per riproducibilit√†
    
    Args:
        seed: Valore seed
    """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


=== END OF FILE src/utils/seed.py ===


=== START OF FILE src/utils/vram.py ===
Percorso: src/utils/vram.py
Dimensione: 618 bytes
Ultima modifica: 2025-12-31 10:31:58.093613917 +0000

"""
Utilities per monitoraggio VRAM
"""
import torch
from typing import Dict


def get_vram_usage() -> Dict[str, float]:
    """Ottiene uso VRAM corrente in GB"""
    if not torch.cuda.is_available():
        return {"allocated": 0.0, "reserved": 0.0, "max_allocated": 0.0}
    
    return {
        "allocated": torch.cuda.memory_allocated() / 1e9,
        "reserved": torch.cuda.memory_reserved() / 1e9,
        "max_allocated": torch.cuda.max_memory_allocated() / 1e9
    }


def reset_peak_memory():
    """Reset peak memory stats"""
    if torch.cuda.is_available():
        torch.cuda.reset_peak_memory_stats()


=== END OF FILE src/utils/vram.py ===


=== ARCHITETTURA SISTEMA ===

üéØ OBIETTIVI:
  - Compressione contesti molto lunghi: Fino a ~262k token in sequenze latenti corte
  - Efficienza computazionale: Teacher grande legge tutto, Reasoner piccolo genera risposta
  - Training end-to-end: Compressore addestrato con distillation da Teacher
  - Baseline comparison: Confronto con RAG, Summary, Pooling

‚ú® COMPONENTI PRINCIPALI:
  1. Teacher/Reader (Qwen3 30B MoE AWQ):
     - Legge contesto completo fino a 262k token
     - Disponibile su vLLM porta 8000
     - Estrae hidden states da layer specifici (default: -6)
     - Fallback: logits-only o proxy model se AWQ non supporta hidden states

  2. Compressore (Resampler Perceiver-style):
     - Comprime hidden states in sequenze latenti (N_lat tokens)
     - Supporta resampler globale opzionale (n_latents_global)
     - Architettura: n_compressor_blocks, n_heads, ff_dim configurabili
     - Output: token latenti per Reasoner

  3. Reasoner (Qwen3 4B):
     - Consuma solo (latenti + domanda) invece di contesto completo
     - Usa Transformers per supportare inputs_embeds
     - Genera risposta finale

  4. Baseline Systems:
     - Teacher full context: Baseline upper bound
     - RAG top-k chunks: Retrieval top-k chunks rilevanti
     - Summary per chunk: Summary di ogni chunk
     - Pooling latents: Mean/max pooling su hidden states

=== FLUSSO DI ESECUZIONE ===

1. GENERAZIONE DATASET:
   python -m src.data.generate_synth --config configs/base.yaml
   - Genera dataset sintetico con contesti lunghi
   - Configurabile: train_size, val_size, test_size, num_keys, noise_ratio
   - Range contesto: min_context_tokens - max_context_tokens

2. TRAINING COMPRESSORE:
   python -m src.train.train_compressor --config configs/base.yaml --exp S0
   - Training loop con distillation da Teacher
   - Configurabile: lr, batch_size, gradient_accumulation_steps, max_steps
   - Checkpoint salvati in runs/

3. VALUTAZIONE:
   python -m src.eval.run_eval --config configs/base.yaml --exp S0
   - Valutazione su test set
   - Confronto con baseline
   - Genera report.json

4. REPORT:
   python -m src.eval.report --input outputs/report.json
   - Genera plot e statistiche
   - Output in outputs/

=== ESPERIMENTI CONFIGURATI ===

S0 - Sanity Check:
  - Contesto: 32k token
  - N_lat: 1024
  - Global resampler: false
  - Max steps: 2000

S1_4096 - Main 4096:
  - Contesto: 256k token
  - N_lat: 4096
  - Global resampler: true
  - Max steps: 5000

S1_8192 - Main 8192:
  - Contesto: 256k token
  - N_lat: 8192
  - Global resampler: true
  - Max steps: 5000

S1_1024 - Main 1024:
  - Contesto: 256k token
  - N_lat: 1024
  - Global resampler: true
  - Max steps: 5000

=== STRUTTURA PROGETTO ===

latent-compress-llm/
‚îú‚îÄ‚îÄ configs/          # Configurazioni YAML
‚îÇ   ‚îú‚îÄ‚îÄ base.yaml     # Config base
‚îÇ   ‚îî‚îÄ‚îÄ exp_grid.yaml # Griglia esperimenti
‚îú‚îÄ‚îÄ data/             # Dataset e cache
‚îú‚îÄ‚îÄ src/              # Codice sorgente
‚îÇ   ‚îú‚îÄ‚îÄ data/         # Dataset e generazione
‚îÇ   ‚îú‚îÄ‚îÄ models/       # Modelli: compressor, teacher, reasoner, baseline
‚îÇ   ‚îú‚îÄ‚îÄ train/        # Training loop e distillation
‚îÇ   ‚îú‚îÄ‚îÄ eval/         # Valutazione e report
‚îÇ   ‚îî‚îÄ‚îÄ utils/        # Utilities: logging, metrics, tokenization, etc.
‚îú‚îÄ‚îÄ runs/             # Checkpoint training
‚îú‚îÄ‚îÄ outputs/          # Report e plot
‚îú‚îÄ‚îÄ README.md         # Documentazione principale
‚îî‚îÄ‚îÄ requirements.txt  # Dipendenze Python

=== ASSUNZIONI E LIMITAZIONI ===

‚úÖ ASSUNZIONI:
  - Teacher disponibile su vLLM porta 8000
  - Se estrazione hidden states fallisce con AWQ, si usa fallback logits-only o proxy model
  - Reasoner usa Transformers per supportare inputs_embeds

‚ö†Ô∏è  LIMITAZIONI:
  - Teacher AWQ potrebbe non supportare estrazione hidden states
  - Compressione introduce perdita di informazione
  - Training richiede GPU con VRAM sufficiente

=== CONFIGURAZIONE COMPRESSORE ===

Parametri principali:
  - n_latents: Numero token latenti (default: 4096)
  - d_lat: Dimensione embedding latenti (default: 512)
  - n_latents_global: Token latenti globali opzionali (default: 8192)
  - use_global_resampler: Abilita resampler globale (default: true)
  - n_compressor_blocks: Numero blocchi transformer (default: 2)
  - n_heads: Numero attention heads (default: 8)
  - ff_dim: Dimensione feed-forward (default: 2048)

Layer hidden states:
  - layer_indices: Indici layer da cui estrarre hidden states (default: [-6])

=== STATISTICHE PROGETTO ===
Data generazione: Wed Dec 31 10:58:05 UTC 2025
Totale file processati: 28
File trovati: 28
File mancanti: 0
Dimensione totale codice: 85 KB

=== COMPONENTI INCLUSI ===
‚úÖ Data Module - Dataset loader e generazione sintetica
‚úÖ Models Module - Compressore, Teacher, Reasoner, Baseline
‚úÖ Training Module - Training loop e distillation
‚úÖ Evaluation Module - Valutazione e report generation
‚úÖ Utils Module - Logging, metrics, tokenization, timers, seed, vram
‚úÖ Configuration - Config YAML per base e esperimenti
‚úÖ Documentation - README principale
‚úÖ Dependencies - requirements.txt completo

=== DESIGN PRINCIPLES ===
1. Modularit√†: Componenti separati e riutilizzabili
2. Configurabilit√†: Tutto configurabile via YAML
3. Baseline comparison: Confronto sistematico con metodi alternativi
4. End-to-end training: Training completo con distillation
5. Scalabilit√†: Supporto contesti molto lunghi (fino a 262k token)
6. Efficienza: Reasoner piccolo consuma solo latenti compressi

=== DIPENDENZE PRINCIPALI ===
  - torch>=2.0.0: PyTorch per training e inference
  - transformers>=4.40.0: Hugging Face Transformers
  - vllm>=0.4.0: vLLM per Teacher inference
  - accelerate>=0.30.0: Accelerate per training distribuito
  - datasets>=2.16.0: Hugging Face Datasets
  - einops>=0.7.0: Operazioni tensor eleganti
  - autoawq>=0.2.0: AutoAWQ per quantizzazione
  - rank-bm25>=0.2.2: BM25 per baseline RAG

--- END OF FILE latent_compress_llm_completo_001.txt ---
